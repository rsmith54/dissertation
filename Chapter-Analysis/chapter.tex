%This is the first chapter of the dissertation

%The following command starts your chapter. If you want different titles used in your ToC and at the top of the page throughout the chapter, you can specify those values here. Since Columbia doesn't want extra information in the headers and footers, the "Top of Page Title" value won't actually appear.

\chapter[A search for supersymmetric particles in zero lepton final states with the Recursive Jigsaw Technique][Top of Page Title]{A search for supersymmetric particles in zero lepton final states with the Recursive Jigsaw Technique}

This section presents the details of the first search employing RJR variables as discriminating variables, as described in ~\cite{ATLAS-CONF-2016-078}.
We will describe the simulation samples used, and then define the selections where we search for new SUSY phenomena, which we call the \textit{signal regions} (SRs)
Afterwards, we describe the background estimation techniques used in the analysis.
Finally, we discuss the treatment of systematic uncertainties, and how we combine them using a likelihood method~\cite{Baak:2014wma}.

\section{Simulation samples}

We discussed the collision data sample provided by the LHC for the analysis in this thesis.
We analyze a dataset of 13.3 \ifb of collision data, at $\sqrt{s} = 13 \TeV$.
To select events in data, we use the trigger system as previously discussed, and use the lowest unprescaled trigger which is available for a particular Standard Model background.
We now discuss the simulation samples used for this search.

Simulated data is fundamentally important to the ATLAS physics program.
Calibrations, measurements, and searches use Monte Carlo (MC) simulations\footnotemark~ to compare with collision data.
\footnotetext{In jargon, often just called ``Monte Carlo'' or MC.}
In this thesis, MC samples are used to optimize the signal region selections, assist in background estimation, and assess the sensitivity to specific SUSY signal models.
The details of Monte Carlo production, accuracy, and utility are far beyond the scope of this thesis, but we provide a short description here.

The first step is MC \textit{generation}.
A program is run which does a matrix-element calculation, sometimes with additional corrections, which produces a set of output particles from the parton interactions.
These output particles are then decayed via another (or the same) simulation program.
This produces a set of \textit{truth} particles, which are the output of event generation.
The details of which generator to use are the subject of much discussion, and generally (many) comparisons are made between them, for different processes of interest.
Additionally, differences between generators are often a starting point for the calculation of systematic uncertainties.

The next step is the \textit{simulation}.
The detector response to the truth particles is simulated, and simulated hits are produced.
After simulation, the standard reconstruction algorithms described previously are run with the simulated hits.
This procedure ensures ``as close as possible'' treatment of simulation and collision data.

We give a brief description of which samples use which generators; additional details are available in ~\cite{ATLAS-CONF-2016-078}.

\todo{MAKE BETTER}
{\color{red}
Signal (digluino and disquark) samples are generated with up to two extra partons in the matrix element using MG5\_aMC@NLO~2.2.2 event generator~\cite{Alwall:2014hca} interfaced to  \pythia~8.186~\cite{Sjostrand:2014zea}.
The nominal cross-section is taken from an envelope of cross-section predictions using different PDF sets and factorization and renormalization scales, as described in Ref.~\cite{Kramer:2012bx}, considering only light-flavour quarks ($u$, $d$, $s$, $c$).
For the light-flavour squarks (gluinos) in case of gluino- (squark-) pair production, cross-sections are evaluated assuming masses of 450 \TeV.
The free parameters are $m_{\lsp}$ and $m_{\gluino}$ ($m_{\squark}$) for gluino-pair (squark-pair) production models.
\todo{explain we have a ``grid'' of these signal models samples}
%The {\textsc EvtGen}~v1.2.0 program~\cite{evtgen} is used to describe the properties of the $b$- and $c$- hadron decays in the signal samples, and the background samples except those produced with \sherpa~\cite{sherpa2}.

Boson ($W$, $Z$, $\gamma$) plus jet events are simulated using different \sherpa generators, with \textsc{Comix} and \textsc{OpenLoops} matrix-element generators~\cite{ATL-PHYS-PUB-2016-003, comix, openloops}.
Photons are required to have transverse momentum of $> 35 \GeV$.
Importantly, the $W (Z)$+jet events are calculated at NLO while the the $\gamma$+jet events are calculated at LO.
% The production of $W$ or $Z$ bosons in association with jets~\cite{ATL-PHYS-PUB-2016-003} is simulated using the \sherpa~2.2.0 generator, while the production of $\gamma$ in association with jets is simulated using the \sherpa~2.1.1 generator.
%In events with $W$ or $Z$ bosons, the matrix elements calculated using the , and merged with the \sherpa\ parton shower \cite{sherpashower} using the ME+PS@NLO prescription \cite{mepsnlo}. %To fix the scale setting problem, a reweighting procedure based on the number truth jets is applied.
% The samples are produced with a simplified scale setting prescription in the multi-parton matrix elements, to improve the event generation speed. A theory-based re-weighting of the jet multiplicity distribution is applied at event level, derived from event generation with the strict scale prescription.
% Events containing a photon in association with jets are generated requiring a photon transverse momentum above 35~\GeV.
%  For these events, matrix elements are calculated at LO with up to three or four partons depending on the $\pt$ of the photon, and merged with the \sherpa\ parton shower using the ME+PS@LO prescription~\cite{Hoeche:2009rj}.
%In the case of $W/Z$+jets, the NNPDF3.0NNLO PDF set \cite{Ball:2014uwa} is used, while for the $\gamma$+jets production the CT10 PDF set \cite{CT10pdf} is used, both in conjunction with dedicated parton shower-tuning developed by the authors of \sherpa.
The $W/Z$ + jets events are normalized to their NNLO cross-sections ~\cite{Catani:2009sm}.
The $\gamma$+jets LO cross-section is taken directly from \sherpa; we will apply a correction factor to be described later.

The various \ttbar and single-top processes~\cite{ATL-PHYS-PUB-2016-004} are generated using two versions of \textsc{Powheg-Box} ~\cite{ATL-PHYS-PUB-2016-004,powheg-box}.
These are calculated at NLO and normalized to various orders ranging from NLO to NNLO+NNLL in the different processes, which can be seen in \ref{tab:montecarlo}\cite{Czakon:2013goa,Czakon:2011xx,Aliev:2010zk,Kant:2014oha,Kidonakis:2010ux,Kidonakis:2011wy}.
% For the generation of $t\bar{t}$ and single-top processes in the $Wt$ and $s$-channel~\cite{ATL-PHYS-PUB-2016-004}, the \textsc{Powheg-Box} v2 \cite{powheg-box} generator is used with the CT10 PDF set.
% The electroweak (EW) $t$-channel single-top events are generated using the \textsc{Powheg-Box} v1 generator.
% This generator uses the four-flavour scheme for the NLO matrix-element calculations together with the fixed four-flavour PDF set CT10f4~\cite{CT10pdf}.
% For this process, the decay of the top quark is simulated using {\textsc MadSpin} tool \cite{10a} preserving all spin correlations, while for all processes the parton shower, fragmentation, and the underlying event are generated using \pythia~6.428 \cite{pythia6} with the CTEQ6L1 \cite{Pumplin:2002vw} PDF set and the corresponding {\textsc Perugia 2012} tune (P2012) \cite{perugia}. The top quark mass is set to 172.5~\GeV.
% The $h_{\rm damp}$ parameter, which controls the $\pt$ of the first additional emission beyond the Born configuration, is set to the mass of the top quark. The main effect of this is to regulate the high-$\pt$ emission against which the ttbar system recoils \cite{ATL-PHYS-PUB-2016-004}.
%The EvtGen v1.2.0 program \cite{evtgen} is used for properties of the bottom and charm hadron decays.
% The $t\bar{t}$ events are normalized to the NNLO+NNLL ~\cite{Czakon:2013goa,Czakon:2011xx}.
% The $s$- and $t$-channel single-top events are normalized to the NLO cross-sections \cite{}, and the $Wt$-channel single-top events are normalized to the NNLO+NNLL~\cite{}.
% For the generation of $t\bar{t}$ + EW processes ($t\bar{t} + W/Z/WW$)~\cite{ATL-PHYS-PUB-2016-005}, the MG5\_aMC@NLO~2.2.3~\cite{Alwall:2014hca} generator at LO interfaced to the \pythia~8.186 parton-shower model is used, with up to two ($t\bar{t}+W$, $t\bar{t}+Z(\to \nu\nu/qq)$), one ($t\bar{t}+Z(\to \ell\ell)$) or no ($t\bar{t}+WW$) extra partons included in the matrix element.
% The ATLAS underlying-event tune A14 is used together with the NNPDF2.3LO PDF set.
% The events are normalized to their respective NLO cross-sections~\cite{Lazopoulos:2008de,Campbell:2012dh}.

Diboson processes ($WW$, $WZ$, $ZZ$)~\cite{ATL-PHYS-PUB-2016-002} are simulated using the  \sherpa~2.1.1 generator
For processes with four charged leptons (4$\ell$), three charged leptons and a neutrino (3$\ell$+1$\nu$) or two charged leptons and two neutrinos (2$\ell$+2$\nu$), the matrix elements contain all diagrams with four electroweak vertices, and are calculated for up to one (4$\ell$, 2$\ell$+2$\nu$) or no partons (3$\ell$+1$\nu$) at NLO and up to three partons at LO using the \textsc{Comix} and \textsc{OpenLoops} matrix-element generators, and merged with the \sherpa\ parton shower using the ME+PS@NLO prescription.
For processes in which one of the bosons decays hadronically and the other leptonically, matrix elements are calculated for up to one ($ZZ$) or no ($WW$, $WZ$) additional partons at NLO and for up to three additional partons at LO using the \textsc{Comix} and \textsc{OpenLoops} matrix-element generators, and merged with the \sherpa\ parton shower using the ME+PS@NLO prescription.
In all cases, the CT10 PDF set is used in conjunction with a dedicated parton-shower tuning developed by the authors of  \sherpa.
The generator cross-sections are used in this case.

The multi-jet background is generated with \pythia~8.186 using the A14 underlying-event tune and the NNPDF2.3LO parton distribution functions.

A summary of the SM background processes together with the MC generators, cross-section calculation orders in $\alpha_{\textrm s}$, PDFs, parton shower and tunes used is given in Table~\ref{tab:montecarlo}.
}
\input{Chapter-Analysis/mc-samples}

%The summary of processes considered together with the MC generators, cross-section calculations and PDFs used are listed in Table~\ref{tab:montecarlo}.
For all SM background samples the response of the detector to particles is modelled with a full ATLAS detector simulation ~\cite{:2010wqa} based on \textsc{Geant4} ~\cite{Agostinelli:2002hh}.
Signal samples are prepared using a fast simulation based on a parameterization of the performance of the ATLAS electromagnetic and hadronic calorimeters ~\cite{ATLAS:2010bfa} and on \textsc{Geant4} elsewhere.
%, or through a fast simulation using a parameterization of the performance of the ATLAS electromagnetic and hadronic calorimeters ~\cite{ATLAS:2010bfa} and \textsc{Geant4} elsewhere; the latter applies to  \powheg{}+\pythia{} $t\bar{t}$ samples.

All simulated events are overlaid with multiple $pp$ collisions simulated with the soft QCD processes of \pythia~8.186 using the A2 tune  ~\cite{A14tune} and the MSTW2008LO parton distribution functions ~\cite{Martin:2009iq}. The simulations are reweighted to match the distribution of the mean number of interactions observed in data. %It was checked that the effect of such pile-up reweighting is completely negligible.

%The MC samples were generated with an expected pile-up distribution (multiple $pp$ interactions in the same or neighbouring bunch-crossings), but have not been reweighted to match the distribution of the mean number of interactions observed in data. It has been checked that the effect of such pile-up reweighting is completely negligible.

%Differing pile-up (multiple $pp$ interactions in the same or neighbouring bunch-crossings) conditions as a function of the instantaneous luminosity are taken into account by overlaying simulated minimum-bias events (simulated using \pythia~8 with the MSTW2008LO PDF set~~\cite{Sherstnev:2007nd} and the A2 tune ~\cite{ATL-PHYS-PUB-2011-014})  onto the hard-scattering process and reweighting events according to the distribution of the mean number of interactions observed in data.

\section{Event selection}

This section describes the selection of the signal region events.
We begin by describing the \textit{preselection}, which is used to remove problematic events and reduce the dataset to a manageable size.
We then describe the signal region strategy, and present the signal regions used in the analysis.

\subsection{Preselection}

The preselection is used to reduce the dataset to that of interest in this thesis.
The table containing the preselection cuts is shown in \ref{tab:preselection}.
This selection is also used for the samples used for background estimation, except for the lepton veto.

The cuts [1] and [4] are a set of cleaning cuts to remove problematic events.
The \textit{Good Runs List} is a centrally-maintained list of data runs which have been determined to be ``good for physics''.
This determination is made by analysis of the various subdetectors, and monitoring of their status.
Event cleaning is used to veto events which could be affected by noncollision background, noise bursts, or cosmic rays.


We require the lowest unprescaled \met trigger for the data run of interest, as described previously, in cut [2].
The lepton veto is applied in cut [5].
These two cuts are only used for the signal region selection.

The rest of the preselection is used for the signal region and control regions used for background estimation.
These cuts on scaleful variables used by previous searches are mostly used for the reduction of the dataset to a manageable size.
Signal models with sensitivity to lower values of these scaleful variables have been ruled out by previous searches~\cite{SUSY-2015-06}.
The final cut is on \meff, which is the scalar sum of all jets and \met.
This is the final discriminating variable used in the complementary search to this thesis, which is also presented in ~\cite{ATLAS-CONF-2016-078}.

\input{Chapter-Analysis/preselection-table}

\subsection{Signal regions}
We define a set of of signal regions using the RJR variables previously described.
These signal regions are split into three general categories: squark pair production SRs, gluino pair production SRs, and compressed production SRs.
Within these general SRs, we have a set of signal regions targetting different mass splittings of the sparticle and LSP.
\begin{figure}
\caption{Schematic leading the development of the SUSY signal regions in this thesis.
A variant of this schematic is used for most SUSY searches on ATLAS and CMS.
} \label{fig:sr_schematic}
\includegraphics[width=.9\linewidth]{sr_schematic}
\end{figure}

A schematic of this strategy is shown in \ref{fig:sr_schematic}.
This type of plane is how most $R-$parity conserving) SUSY searches are organized in both ATLAS and CMS
The horizontal axis is the mass of the sparticle considered.
In the case of this thesis, this will the squark or gluino mass.
On the horizontal axis, we place the LSP mass.
These are the two free parameters of the simplified models considered here.
Our search occurs in this two-parameter space.
Each signal region targets some portion of this plane.
As shown in the figure, a new iteration of a search will use a set of signal regions which have sensitivity just beyond those of the previous exclusions.
The choice of how many signal regions to use to fully cover this plane is in many ways a matter of judgment, as it is important to avoid over or under/over-fitting to the signal models of interest.
To take the extreme examples, One signal region will obscure the different phenomena in signal events with large versus small mass splittings, leading to underfitting.
Binning as finely as possible\footnotemark~ leads to overfitting due to the fluctuations present in the signal and background events passing the various selections selection.
\footnotetext{This can be defined as having a signal region for each simulated signal sample, which for this analysis is \order 100.}
In this thesis, we use six squark signal regions, six gluino signal regions, and five compressed regions.

The full table defining all signal regions is shown in \ref{tab:RJsrdefs}.
In all cases, the signal region selections contain a combination of scaleful and scaleless cuts.
Emphasis on cuts on scaleful variables provide stronger sensitivity to larger mass splittings, while additional sensitivity to smaller mass splittings is found using stronger cuts on scaleless variables.
One envisions walking from SR1 (with tight scaleless cuts and loose scaleful cuts) in \ref{fig:sr_schematic} towards SR3 by loosening the scaleless cuts and tightening the scaleful cuts.
We will see this strategy at work in each set of signal regions.

We have already described the useful variables in the previous chapter.
The question is how to choose the optimal cuts for a given set of signal models, which are grouped in the mass splitting space.
This was done by a brute force scan over the cut values, using a guess of integrated luminosity with a fixed systematic uncertainty scenario; the value of the systematic uncertainty is motivated by that from previous analyses.
We choose the lowest cut value that maximizes the $Z_{\text{Bi}}$, as described in ~\cite{Cousins:2008zz}.
This figure of merit gives conservative estimates, as compared to i.e. $S/\sqrt{B}$.
A figure showing an example of this selection tuning procedure is shown in \ref{fig:sr_optimization}.

\begin{figure}
\caption{Optimization of the \HTFnm{PP}{4}{1} cut for a gluino signal model with $(m_{\gluino}, m_{\lsp} ) = (1500,700) \GeV $ assuming 10 \ifb and an uncertainty of 20\% on the background estimate.
} \label{fig:sr_optimization}
\includegraphics[width=.9\linewidth]{ATLAS-CONF-2016-078_INT/OPT_gluino/HT5PP_10fb_20sys_gg1500_700}
\end{figure}

The compressed selections are split into five regions (SRC1-5), and due to the simplified nature of the compressed decay tree, has sensitivity in both the gluino and squark planes.
The compressed regions target mass splittings with $m_{\text{sparticle}} - m_{\text{LSP}} \tilde{<} 200 \GeV$.
For the compressed region, $M_{T, S}$ is the primary scaleful variable.
We can see the general strategy of lowering increasing scale cuts while decreasing the scaleless cuts here.
SRC1 targets the most compressed scenarios, with mass splittings of less than 25 \GeV, and has the loosest $M_{T, S}$ cut coupled with the tightest \risr and \dphiISR cuts.
SRC4 and SRC5 target mass splittings of $\tilde{~} 200 \GeV$, and are coupled with the loosest scaleless cuts on \risr and \dphiISR.
We also note that SRC4 and SRC5 have differing cuts on \NVjet, since these SRs are closest to the noncompressed regions, and can be see as the ``crossover'' where the differences between squark and gluino production begins to become manifest.

The squark regions (for noncompressed spectra) are organized into six signal regions.
These are labeled by a numeral 1-3 and letter a/b.
SRs sharing a common numeral i.e. SRS1a and SRS1b share a common set of scaleless cuts, while differing in the main scale variable \HTFnm{PP}{2}{1}.
The two SRs for each set of scaless cuts, only differing in the main scale variable, can be seen in a n{\"a}ive way as providing sensitivity to a range of luminosity scenarios\footnotemark.
\footnotetext{These SRs were defined before the entire collision dataset was produced, and thus needed to be robust in cases where the LHC provided significantly different than expected performance.}
As before, we see that the scaleless cuts are loosened as we tighten the scaleful cuts, as we move across the table from SRS1a to SRS3b.
This provides strong sensivity to signal models with intermediate mass splittings with SRS1a to large mass splittings with SR3b.

The gluino signal regions are organized entirely analogously to the squark signal regions.
There are six gluino signal regions, again labeled via a numeral 1-3 and letter a/b.
Those SRs sharing a common numeral have a common set of scaleless cuts, but differ in their main scale variable \HTFnm{PP}{4}{1}.
The SRs follow scaleless vs scaleful strategy, with SRG1 having the loosest scaleful cut cuts coupled with the strongest scaleless cuts, and the converse being true in SRG3.
As in the squark case, this strategy provides strong expected sensitivity throughout the gluino-LSP plane.

\input{Chapter-Analysis/signal-region-table}

\section{Background estimation}

We describe here the method of background estimation.
In this thesis, we detail what is colloquially called a ``cut-and-count'' analysis.
This is in contrast to a ``shape fit'' analysis, where one needs to consider the details of the variable distribution shapes.
Instead, we must ensure the overall normalization of the Standard Model backgrounds are correct in the regions of phase space considered in the analysis.
In order to do this, we define a set of \textit{control regions} which are free of SUSY contamination based on the previously excluded analysis.
We compare the number of events present in the control regions in simulation with that in data to define a \textit{transfer factor} (TF).
We extrapolate the number of expected events from each background using this transfer factor to translate from the , which provides our final estimate of the SM background in the corresponding signal region.
To be explicit, each signal region SR has a corresponding set of control regions.

More precisely, for a given signal region, we are attempting to estimate the value $\ntext{SR}{data}$ for a given background.
This value is estimated using the following equation:
\begin{align}\label{eq:bkg_est}
\ntext{SR}{data,est} = \ntext{CR}{data,obs} \times \text{TF}_{\text{CR}} \equiv \ntext{CR}{data,obs} \times  \begin{pmatrix} \frac{ \ntext{SR}{MC} }{ \ntext{CR}{MC} } \end{pmatrix}
\end{align}
where the transfer factor TF is taken directly from MC.
The two ingredients to our estimation of \ntext{SR}{data,obs} is thus \ntext{CR}{data,obs} and the transfer factor taken from MC.

The transfer factor method is potentially more straightforward written in the following way:
\begin{align}\label{eq:bkg_est_simple}
\ntext{SR}{data,est} = \ntext{SR}{MC} \times  \begin{pmatrix} \frac{\ntext{CR}{data,obs}  }{ \ntext{CR}{MC} } \end{pmatrix} \equiv \ntext{SR}{MC} \times \mu_{CR}.
\end{align}
In this form, the correction to the overall normalization is explicit.
The ratio $\frac{\ntext{CR}{data,obs}}{ \ntext{CR}{MC} }$ which we call $\mu$ informs us how to scale \ntext{SR}{MC} in order to get the right overall normalization.
The assumption made with this method is that the overall shape of the distribution should not change ``that much'' as one extrapolates to the signal region.

The CR definitions are motivated and designed according to two (generally competing) requirements:
\begin{enumerate}
\item Statistical uncertainties due to low CR statistics
\item Systematic uncertainties related to the extrapolation from the CR to the SR.  This motivates the desire to make the control regions as similar as possible to the signal regions without risking signal contamination while ensuring high purity in the targeted SM background.
\end{enumerate}
In principle, one can also apply data-driven corrections to the TF obtained for each CR.

In order to validate the transfer factors obtained from MC, we also develop a series of \textit{validation regions} (VRs).
These regions are generally designed to be ``in between'' the control region and signal region selections in phase space, and thus provide a check on the extrapolation from the control regions into the signal regions.
Despite their closeness in phase space to the signal regions, they are also designed to have low signal contamination.

In practice, we perform this estimation procedure simultaneously across all control regions; we describe this later.
We only note this here since we can also apply Eq.\ref{eq:bkg_est} to measure the contamination of a control region with another background as well.
This procedure accounts for the correlations between regions due to correlated systematic uncertainties.
We next describe the control region selection for the major SM backgrounds for the analysis.

\subsection{Control Regions}

As was hinted at in the discussion of Monte Carlo generators, the primary backgrounds of note in this analysis are \zjets, \wjets, \ttbar, and QCD events.
There is also a minor background from diboson events which is taken directly from MC with an uncertainty of 50\%.
We describe the strategy to estimate these various backgrounds here.
A summary table is shown in \ref{tab:crdefs}.
All distributions shown in this section use the scaling factors $\mu$ from the background fits, which we describe later.
\input{Chapter-Analysis/background-estimation-table}

Events with a $Z$ boson decaying to neutrinos in association with jets are the primary irreducible background in the analysis.
These events have true \met from the decaying neutrinos, and can have significant values of the scaleful variables of interest.
Naively, one might expect us to use \Zll as the control process of interest, as \Zll events are quite well-measured.
Unfortunately, the \Zll branching ratio is about half of from \Zvv, which necessitates loosening the control region selection significantly.
This leads to unacceptably large systematic uncertainties in the transfer factor.

Instead, photon events are used as the control region for the \Zvv events.
We label this photon control region as CRY.
The photon is required to have $\pt > 150 \GeV$ to ensure the trigger is fully efficient.
The kinematic properties of photon events strongly resemble those of $Z$ events when the boson \pt is significantly above the mass of the $Z$ boson.
In this regime, the neutral bosons are both scaleless, and can be treated interchangeably, up to the differences in coupling strengths.
%There are some residual effects due to the differing spin states, which should be identifiable in events with
Additionally, the cross-section for \gammajets~ events is significantly larger than \zjets~ events above the $Z$ mass.
These features are shown in \ref{fig:boson_pt_ratio} in simulated \Zvv truth and reconstructed events.
The reconstructed \Zvv events define the boson \pt as simply the \met.
In truth events, one clearly sees the effect of the $Z$ mass below \order 100 \GeV, with a flattening of the ratio above \order 300 \GeV.
In reconstructed events, the effects are less clear at low boson \pt, primarily due to cut sculpting from i.e. the trigger requirement on photon events, which necessitates a higher \pt cut on photon events for the trigger to remain fully efficient.
Still, it is clear that the ratio flattens out at high boson \pt, and we are justified in the use of CRY to model the \zjets~ background.

The CRY kinematic selection is slightly looser in the scaleful variables for the noncompressed regions to provide sufficient control region statistics.
This is chosen to be $\HFnm{PP}{1}{1} > 900 \GeV$ ($\HFnm{PP}{1}{1} > 550 \GeV$) for the squark (gluino) regions to minimize the corresponding statistical and systematic uncertainties.
\begin{figure}
\caption{} \label{fig:boson_pt_ratio}
\subfloat[Boson \pt ratio as a function of true boson \pt]{\includegraphics[width=.45\linewidth]{ATLAS-CONF-2016-078_INT/GammaReweighting/Znunu_truth_bosonPt_dPhi/c1_bosonPt_no_cuts}}
\subfloat[Boson \pt ratio as a function of reconstructed boson \pt]{\includegraphics[width=.45\linewidth]{ATLAS-CONF-2016-078_INT/GammaReweighting/Znunu_reco/c1_bosonPt_no_cuts}}
\end{figure}

One additional correction scale factor is applied to \gammajets~ events before calculating the transfer factors.
This is known as the $\kappa$ method, which is used to determine the disagreement arising from the use of a LO generator for photon events vs. a NLO generator for \zjets~ events, which can reduce the theoretical uncertainties from this disagreement.
One can see this as a measurement of the k-factor for the LO \gammajets~ sample.
This is effectively done with an auxiliary CRZ region, defined using two leptons with an invariant mass close with 25 \GeV of the Z mass.
The correction factor derived for this purpose is $\kappa = 1.39 \pm 0.05$.

Distributions of CRY in squark, gluino, and compressed regions are shown in \ref{fig:CRY_SRJigsawSRS1a_LastCut_CRY_minusone,fig:CRY_SRJigsawSRG1a_LastCut_CRY_minusone,fig:CRY_SRJigsawSRC1_LastCut_CRY_minusone}.
One can see the quite high purity of CRY in photon events from these plots.

Event with a $W$ boson decaying leptonically via \wln can also enter the signal region.
In this case, we use leptonically to include all leptons ($e,\mu,\tau$).
The \wjets~ events passing the event selection either have a hadronically-decaying $\tau$, with a neutrion supplying \met, or the case where a muon or electron is misidentified as a jet or missed completely due to the limited detector acceptance.
To model this background, we use a sample of one-lepton events with a veto on b-jets, which we label CRW.
The lepton is required to have $\pt > 27 \GeV$ to guarantee a fully efficient trigger.
We then treat this single lepton as a jet for purposes of the RJR variable calculations.
We apply a kinematic selection on the transverse mass:
\begin{align}
m_T = \sqrt{2p_{T,\ell} \met ( 1 - \cos{ \phi_{e} - \metphi } },
\end{align}
around the $W$ mass: $30 \GeV < m_T < 100 \GeV$.
Checks in simulation shows that these requirements give a sample of high purity \wln background.
Due to low statistics using the kinematic cuts imposed in the signal regions, the control region kinematic cuts are slightly loosened with respect to the signal region cuts.
We use the loosest cut in any signal region as the control region selection for all signal regions.
More clearly, the control region selection corresponding to each signal region is the \textit{same}.
As discussed above, this leads to a tolerable increase in the systematic uncertainty from the extrapolation from the CR to the SR when compared to the resulting statistical uncertainty.

Distributions of CRW in squark, gluino, and compressed regions are shown in \ref{fig:CRW_SRJigsawSRS1a_LastCut_CRW_minusone,fig:CRW_SRJigsawSRG1a_LastCut_CRW_minusone,fig:CRW_SRJigsawSRC1_LastCut_CRW_minusone}.
There is high purity in \wjets~ events in the control region corresponding to all signal regions.

Top events are also an important background, for the same reasons as the \wjets~ background, due to the dominant top decay channel of $t \rightarrow Wb$.
For a top event to be selected by the analysis criteria, as in the case of \wjets, we expect a $W$ to decay via a $\tau$ lepton which decays hadronically or one a muon or electron to be misidentified as a jet or be outside the detector acceptance.
We are not so worried about hadronic or all dileptonic tops: hadronic \ttbar events generally have low \met (and \HFnm{PP}{1}{1}) so they will not pass the kinematic cuts, while dileptonic \ttbar events have a lower cross-section and good reconstruction efficiency from the two leptons.
We are thus primarily concerned with semileptonic \ttbar events with \met from the neutrino.
To model this background, we use the same selection as the $W$ selection, but require that one of the jets chosen by the analysis has at least one $b$-tag.
This selection has quite high purity, as we expect the \ttbar background to have two $b-$jets.
Thus with the 70\% $b-$tagging efficiency working point used in this analysis, ignoring (small) correlations between the two $b-$tags, we expect to tag one of the $b-$jets greater than 90\% of the time.
As with CRW, we need to loosen the cuts applied to CRT with respect to the signal region in order to gain sufficient expected data statistics.
We use exactly the same scheme; the CRT corresponding to each SR is identical, due to using the loosest set of cuts among the SRs.
This comes at the cost of an increased systematic uncertainty for this extrapolation, but it was determined that this tradeoff resulted in the lowest overall uncertainty.

Distributions of CRT in squark, gluino, and compressed regions are shown in \ref{fig:CRT_SRJigsawSRS1a_LastCut_CRT_minusone,fig:CRT_SRJigsawSRG1a_LastCut_CRT_minusone,fig:CRT_SRJigsawSRC1_LastCut_CRT_minusone}.
There is high purity in top events in the control region corresponding to all signal regions.

The final important background is the QCD background.
As briefly discussed in the previous chapter, QCD backgrounds are difficult, for a few reasons we describe here.
The large cross-section for QCD events means that even very rare extreme mismeasurements can be seen in our signal regions.
However, as these events are very rare, one requires extreme confidence in the tails of the distributions to use simulation as an input for background estimation.
To avoid this, the strategy in these cases is to apply a strong enough cut to expect \textit{zero} QCD events in the signal regions to avoid this issue.
To produce a sample enriched in QCD, which we call CRQ, we reverse the $\Delta_{\mathrm{QCD}}$ and \HFnm{PP}{1}{1} cuts.
This analysis uses the jet smearing method, as described in ~\cite{SUSY-2011-20}.
This is a data-driven method which applies a resolution function to well-measured QCD events, which also an estimate of the impact of the jet energy mismeasurement on \met and subsequently the RJR variables.

Distributions of CRQ in squark, gluino, and compressed regions are shown in \ref{fig:CRQ_SRJigsawSRS1a_LastCut_CRQ_minusone,fig:CRQ_SRJigsawSRG1a_LastCut_CRQ_minusone,fig:CRQ_SRJigsawSRC1_LastCut_CRQ_minusone}.
There is high purity in top events in the control region corresponding to all signal regions.

The final background of note in this background is the diboson background.
This background is estimated directly from simulation.
Due to the low cross-section of electroweak processes, this background is not significant in the signal regions.
We assign a large ad-hoc 50\% systematic on the cross-section, and do not attempt to define a control region for this background.

\input{Chapter-Analysis/cr-lastcut}

\subsection{Validation Regions}

As discussed in general terms above, we define a set of validations regions to ensure we can properly model the particular backgrounds as we move closer to the SRs in phase space.
We define at least one validation region for each major background.

For the most important background \Zvv, we use a series of validation regions.
The primary validation region, which we label as VRZ, is defined by selecting lepton pairs of opposite sign and identical flavor which lie with 25 \GeV~ of the Z boson mass.
This selection has high purity for \Zll events as seen in simulation.
We treat the two leptons as contributions to the \met (as we did with the photon in CRY).
This selection uses the same kinematic cuts as the signal region.
We also define two VRs using the same event selection but looser kinematic cuts, which we label VRZa and VRZb.
VRZa has a loosened selection on \HFnm{PP}{1}{1}, again to the loosest value among the signal regions, as was done for CRW and CRt.
VRZa has a loosened selection on the primary scaleful variable  (\HTFnm{PP}{2}{1} or \HTFnm{PP}{4}{1}), again to the loosest value among the signal regions, as was done for CRW and CRT.
These two validation regions allow us to test the modeling of each of these variables individually, as well as allowing more validation region statistics in the signal regions with tighter cuts on these variables.

For the compressed regions, these $Z$ validation region were found lacking.
The leptons are highly boosted in the compressed case, and the lepton acceptance was quite low due to lepton isolation requirements in \deltaR.
Instead, two fully hadronic validation region were developed for the compressed regions.
The first, VRZc has identical requirements to the signal regions with an inverted requirement on \dphiISR.
From simulation, this region was found to be at least 50\% pure in $Z$ events, which was considered enough to validate this background in this extreme portion of phase space.
For additional validation region statistics, we also developed VRZca, which takes again uses the loosest set of cuts from each signal region.
Note this means that each compressed signal region has an identical VRZca.

The top and $W$ validation regions use the same event selection as the corresponding control regions, as described above.
However, unlike the control regions, these validation regions reimpose the SR scaleful variable selections, to be closer in phase space to the hadronic signal regions.
In the same way as we did for VRZa and VRZb, we also define auxiliary VRs which loosen the cuts on the scale variables.
We define VRTa (VRWa) as VRT (VRW) with the same loosened cut on \HFnm{PP}{1}{1} and VRTb (VRWb) as VRT (VRW) with the same loosened cut on the primary scale variable.

The final set of validation regions are those defined to check the estimation of the QCD background.
VRQ is defined to be identical to the corresponding CRQ, but again we use the full SR region cuts for the scaleful variables.
This selection is then closer to the corresponding signal region to validate the CRQ estimate.
We also define the auxiliary validation regions VRQa and VRQb for the noncompressed signal regions.
In this case, we reimpose one of the two inverted cuts in CRQ with respect to the signal regions, to make each one even closer to the SRs.
In CRQa (CRQb), we reimpose the \HFnm{PP}{1}{1} ($\Delta_{\mathrm{QCD}}$).

For the compressed case, we again define a separate validation region, due to the special kinematics probed.
We construct a validation region which is the same as CRQ, with $.5 < \risr < R_{\text{ISR, SR}}$, where $R_{\text{ISR, SR}}$ is the cut on \risr in the corresponding SR.
Again, this can be seen as probing ``in between'' the CR and SR in phase space.

The results of this validation can be seen in \ref{fig:vr_summary}.
Each bin is \textit{pull} of the validation region corresponding to a particular signal region.
This is defined
\begin{align}
\text{Pull} = \frac{N_{\mathrm{obs}} - N_{\mathrm{pred}}}{\sigma_{\mathrm{tot}}}.
\end{align}
where $\sigma_{\mathrm{tot}}$ is the total uncertainty folding in all systematic uncertainties, which we will describe later.
Assuming we have well-measured our backgrounds, we expect a Gaussian distribution of the pulls around 0, with a standard deviation of 1, as this is measuring the number of standard deviations around the mean.
We can see there are few postive pulls (indicating an underestimation of the background), indicating we have conservatively measured the Standard Model backgrounds with our control regions.
\begin{figure}
\caption{Summary of the validation region pulls} \label{fig:vr_summary}
\includegraphics[width=.9\linewidth]{ATLAS-CONF-2016-078/fig_05b}
\end{figure}

%\subsection{R Z/$\gamma$ method}
\subsection{Systematic Uncertainties}

In this section, we discuss the uncertainties considered.
These generally fall into four categories: theoretical generator uncertainties, uncertainties on the CR to SR extrapolations, uncertainties on the data-driven transfer factor corrections, and object reconstruction uncertainties.
We discuss each of these categories here.
A table summarizing this section is in \ref{tab:systematics-table}
\input{Chapter-Analysis/systematics-table}

The theoretical generator uncertainties are evaluated by using alternative simulation samples or varying scale uncertainties.
In the case of the \zjets~ and \wjets~ backgrounds, the related theoretical uncertainties are estimated by varying the renormalization, factorization, and resummation scales by two, and decreasing the nominal CKKW matching scale by 5 \GeV~ and 10 \GeV~ respectively.
In the case of \ttbar production, we compare the nominal \textsc{Powheg-Box} generator with MG5\_aMC@NLO, as well as comparing different radiation and generator tunes.
As stated above, we account for the uncertainty on the small diboson bacground by imposition of a flat 50\% uncertainty.

The CR to SR extrapolation uncertainties, or what could be called the transfer factor uncertainties, are listed in \ref{tab:systematics-table} as $mu\_$.
There is one normalization factor $\mu$ for each major background, and their uncertainties, especially $\mu_Z$, are often dominant for the measurement in many signal regions.
This uncertainty is generally dominated by the statistical uncertainty in the CR.

There are two uncertainties from the data-driven corrections to the transfer factors.
The first is the uncertainty on $\kappa$, which is measured using an auxiliary \Zll control region.
This is labeled alpha\_Kappa.
The other is the uncertainty is that assigned to the jet smearing method, which is seen in the table as alpha\_QCDError.

The final set of uncertainties are those related to object reconstruction.
In the case of the hadronic search presented, the important uncertainties are those assigned to the jet energy and \met.
The uncertainties on the lepton reconstruction and $b$-tagging uncertainties were found to be negligible in all SRs.
The measurement of the jet energy scale (JES) uncertainty is quite complicated, and described in ~\cite{Aad:2011he,Aad:2012vm,ATL-PHYS-PUB-2015-015}.
After a complicated procedure to decorrelate the various components of the JES uncertainty, there are three components which remain, which are labeled as alpha\_JET\_GroupedNP\_1,2,3.
The jet energy resolution uncertainty is estimated using the methods discussed in Refs.~\cite{Aad:2012ag,ATL-PHYS-PUB-2015-015}, and is labeled alpha\_JER.

The \met soft term uncertainties are described in ~\cite{PERF-2014-04,ATL-PHYS-PUB-2015-023,ATL-PHYS-PUB-2015-027}.
The uncertainty on the \met soft term resolution is parameterized into a component parellel to direction of the rest of the event (the sum of the hard objects \pt) and a component perpendicular to this direction.
There is also an uncertainty on the \met soft term scale.
These are labeled as alpha\_MET\_SoftTrk\_ResoPara, alpha\_MET\_SoftTrk\_ResoPerp, and alpha\_MET\_SoftTrk\_Scale.

\subsection{Fitting procedure}

In this section, we describe the fitting procedure employed, which properly accounts for the correlations between the uncertainties through the use of a likelihood fit as described in ~\cite{Baak:2014wma}.
We use three classes of likelihood fits: \textit{background-only}, \textit{model-independent}, and \textit{model-dependent} fits.
The background-only fits estimate the background yields in each signal region.
These fits use only the control region event yields as inputs; they do not include the information from the signal regions besides the simulation event yield.
The cross-contamination between CRs is also fit by this procedure.
The systematic uncertainties described in the previous section are used as nuisance parameters.
This background only fit also estimates the background event yields in the validation regions.
When designing the analysis (before unblinding the signal regions), checking the validation region agreement is the primary way to validate the consistency and accuracy of the background estimation procedure.

In the case no excess is observed, we use a model-independent fit to set upper limits on the possible number of possible beyond the Standard Model events in each SR.
These limits are derived using the same procedure as the background-only fit, with two additional pieces of information included in the fitting procedure.
We include the SR event count, and a parameter known as the \textit{signal strength}, defined as $\mu = \sigma/\sigma_{\mathrm{BG}}$.
Using the $CL_{\mathrm{S}}$ procedure~\cite{Feldman:1997qc} and neglecting the possible (small) signal contamination in control regions, we derive the the observed and expected limits on the number of events from BSM phenomena in each signal region.

Model-dependent fits are used to set exclusion limits on the specific SUSY models considered in this thesis, particular the gluino or squark pair production with various mass splittings.
This can be seen as identical to the background-only fit with an additional simulation input from the particular model of interest, with its corresponding systematic uncertainties from detector effects accounted for as in the background-only fit.
As noted when we introduced \ref{fig:sr_schematic}, the exclusion contours from previous model-dependent fits are the primary motivating factor in the design of our signal regions.
If no excess is found, we set limits on each of the simplified signal models with various mass splittings.