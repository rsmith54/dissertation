%This is the first chapter of the dissertation

%The following command starts your chapter. If you want different titles used in your ToC and at the top of the page throughout the chapter, you can specify those values here. Since Columbia doesn't want extra information in the headers and footers, the "Top of Page Title" value won't actually appear.

\chapter[A search for supersymmetric particles in zero lepton final states with the Recursive Jigsaw Technique][Top of Page Title]{Title of Chapter 1}

This section presents the details of the first search employing RJR variables as discriminating variables, as described in \ref{ATLAS-CONF-2016-078}.
We will describe the data and simulation samples used, and then define the selections where we search for new SUSY phenomena, which we call the \textit{signal regions} (SRs)
Afterwards, we describe the background estimation techniques used in the analysis.
Finally, we discuss the treatment of systematic uncertainties, and how we combine them using a likelihood method\cite{Baak:2014wma}.

\section{Collision data and simulation samples}

Simulated data is fundamentally important to the ATLAS physics program.
Calibrations, measurements, and searches use Monte Carlo (MC) simulations\footnotemark to compare with collision data.
\footnotetext{In jargon, often just called ``Monte Carlo'' or MC.}
In this thesis, MC samples are used to optimize the signal region selections, assist in background estimation, and assess the sensitivity to specific SUSY signal models.
The details of Monte Carlo production, accuracy, and utility are far beyond the scope of this thesis, but we provide a short description here.

The first step is MC \textit{generation}.
A program is run which does a matrix-element calculation, sometimes with additional corrections, which produces a set of output particles from the parton interactions.
These output particles are then decayed via another (or the same) simulation program.
This produces a set of \textit{truth} particles, which are the output of event generation.
The details of which generator to use are the subject of much discussion, and generally (many) comparisons are made between them, for different processes of interest.
Additionally, differences between generators are often a starting point for the calculation of systematic uncertainties.

The next step is the \textit{simulation}.
The detector response to the truth particles is simulated, and simulated hits are produced.
After simulation, the standard reconstruction algorithms described previously are run with the simulated hits.
This procedure ensures ``as close as possible'' treatment of simulation and collision data.

We give a brief description of which samples use which generators; additional details are available in \ref{ATLAS-CONF-2016-078}.

\todo{MAKE BETTER}
Signal (digluino and disquark) samples are generated with up to two extra partons in the matrix element using MG5\_aMC@NLO~2.2.2 event generator~\cite{Alwall:2014hca} interfaced to  \pythia~8.186~\cite{Sjostrand:2014zea}.
The nominal cross-section is taken from an envelope of cross-section predictions using different PDF sets and factorization and renormalization scales, as described in Ref.~\cite{Kramer:2012bx}, considering only light-flavour quarks ($u$, $d$, $s$, $c$).
For the light-flavour squarks (gluinos) in case of gluino- (squark-) pair production, cross-sections are evaluated assuming masses of 450 \TeV.
The free parameters are $m_{\lsp}$ and $m_{\gluino}$ ($m_{\squark}$) for gluino-pair (squark-pair) production models.
\todo{explain we have a ``grid'' of these signal models samples}
%The {\textsc EvtGen}~v1.2.0 program~\cite{evtgen} is used to describe the properties of the $b$- and $c$- hadron decays in the signal samples, and the background samples except those produced with \sherpa~\cite{sherpa2}.

Boson ($W$, $Z$, $\gamma$) plus jet events are simulated using different \sherpa generators, with \textsc{Comix} and \textsc{OpenLoops} matrix-element generators\cite{ATL-PHYS-PUB-2016-003, comix, openloops}.
Photons are required to have transverse momentum of $> 35 \GeV$.
Importantly, the $W (Z)$+jet events are calculated at NLO while the the $\gamma$+jet events are calculated at LO.
% The production of $W$ or $Z$ bosons in association with jets~\cite{ATL-PHYS-PUB-2016-003} is simulated using the \sherpa~2.2.0 generator, while the production of $\gamma$ in association with jets is simulated using the \sherpa~2.1.1 generator.
%In events with $W$ or $Z$ bosons, the matrix elements calculated using the , and merged with the \sherpa\ parton shower \cite{sherpashower} using the ME+PS@NLO prescription \cite{mepsnlo}. %To fix the scale setting problem, a reweighting procedure based on the number truth jets is applied.
% The samples are produced with a simplified scale setting prescription in the multi-parton matrix elements, to improve the event generation speed. A theory-based re-weighting of the jet multiplicity distribution is applied at event level, derived from event generation with the strict scale prescription.
% Events containing a photon in association with jets are generated requiring a photon transverse momentum above 35~\GeV.
%  For these events, matrix elements are calculated at LO with up to three or four partons depending on the $\pt$ of the photon, and merged with the \sherpa\ parton shower using the ME+PS@LO prescription~\cite{Hoeche:2009rj}.
%In the case of $W/Z$+jets, the NNPDF3.0NNLO PDF set \cite{Ball:2014uwa} is used, while for the $\gamma$+jets production the CT10 PDF set \cite{CT10pdf} is used, both in conjunction with dedicated parton shower-tuning developed by the authors of \sherpa.
The $W/Z$ + jets events are normalized to their NNLO cross-sections \cite{Catani:2009sm}.
The $\gamma$+jets LO cross-section is taken directly from \sherpa; we will apply a correction factor to be described later.\todo{THISSSSSS}

The various \ttbar and single-top processes\cite{ATL-PHYS-PUB-2016-004} are generated using two versions of \textsc{Powheg-Box} \cite{ATL-PHYS-PUB-2016-004,powheg-box}.
These are calculated at NLO and normalized to various orders ranging from NLO to NNLO+NNLL in the different processes, which can be seen in \ref{tab:montecarlo}\cite{Czakon:2013goa,Czakon:2011xx,Aliev:2010zk,Kant:2014oha,Kidonakis:2010ux,Kidonakis:2011wy}.
% For the generation of $t\bar{t}$ and single-top processes in the $Wt$ and $s$-channel~\cite{ATL-PHYS-PUB-2016-004}, the \textsc{Powheg-Box} v2 \cite{powheg-box} generator is used with the CT10 PDF set.
% The electroweak (EW) $t$-channel single-top events are generated using the \textsc{Powheg-Box} v1 generator.
% This generator uses the four-flavour scheme for the NLO matrix-element calculations together with the fixed four-flavour PDF set CT10f4~\cite{CT10pdf}.
% For this process, the decay of the top quark is simulated using {\textsc MadSpin} tool \cite{10a} preserving all spin correlations, while for all processes the parton shower, fragmentation, and the underlying event are generated using \pythia~6.428 \cite{pythia6} with the CTEQ6L1 \cite{Pumplin:2002vw} PDF set and the corresponding {\textsc Perugia 2012} tune (P2012) \cite{perugia}. The top quark mass is set to 172.5~\GeV.
% The $h_{\rm damp}$ parameter, which controls the $\pt$ of the first additional emission beyond the Born configuration, is set to the mass of the top quark. The main effect of this is to regulate the high-$\pt$ emission against which the ttbar system recoils \cite{ATL-PHYS-PUB-2016-004}.
%The EvtGen v1.2.0 program \cite{evtgen} is used for properties of the bottom and charm hadron decays.
% The $t\bar{t}$ events are normalized to the NNLO+NNLL ~\cite{Czakon:2013goa,Czakon:2011xx}.
% The $s$- and $t$-channel single-top events are normalized to the NLO cross-sections \cite{}, and the $Wt$-channel single-top events are normalized to the NNLO+NNLL~\cite{}.
% For the generation of $t\bar{t}$ + EW processes ($t\bar{t} + W/Z/WW$)~\cite{ATL-PHYS-PUB-2016-005}, the MG5\_aMC@NLO~2.2.3~\cite{Alwall:2014hca} generator at LO interfaced to the \pythia~8.186 parton-shower model is used, with up to two ($t\bar{t}+W$, $t\bar{t}+Z(\to \nu\nu/qq)$), one ($t\bar{t}+Z(\to \ell\ell)$) or no ($t\bar{t}+WW$) extra partons included in the matrix element.
% The ATLAS underlying-event tune A14 is used together with the NNPDF2.3LO PDF set.
% The events are normalized to their respective NLO cross-sections~\cite{Lazopoulos:2008de,Campbell:2012dh}.

Diboson processes ($WW$, $WZ$, $ZZ$)~\cite{ATL-PHYS-PUB-2016-002} are simulated using the  \sherpa~2.1.1 generator
For processes with four charged leptons (4$\ell$), three charged leptons and a neutrino (3$\ell$+1$\nu$) or two charged leptons and two neutrinos (2$\ell$+2$\nu$), the matrix elements contain all diagrams with four electroweak vertices, and are calculated for up to one (4$\ell$, 2$\ell$+2$\nu$) or no partons (3$\ell$+1$\nu$) at NLO and up to three partons at LO using the \textsc{Comix} and \textsc{OpenLoops} matrix-element generators, and merged with the \sherpa\ parton shower using the ME+PS@NLO prescription.
For processes in which one of the bosons decays hadronically and the other leptonically, matrix elements are calculated for up to one ($ZZ$) or no ($WW$, $WZ$) additional partons at NLO and for up to three additional partons at LO using the \textsc{Comix} and \textsc{OpenLoops} matrix-element generators, and merged with the \sherpa\ parton shower using the ME+PS@NLO prescription.
In all cases, the CT10 PDF set is used in conjunction with a dedicated parton-shower tuning developed by the authors of  \sherpa.
The generator cross-sections are used in this case.

The multi-jet background is generated with \pythia~8.186 using the A14 underlying-event tune and the NNPDF2.3LO parton distribution functions.

A summary of the SM background processes together with the MC generators, cross-section calculation orders in $\alpha_{\textrm s}$, PDFs, parton shower and tunes used is given in Table~\ref{tab:montecarlo}.

\input{Chapter-Analysis/mc-samples}

%The summary of processes considered together with the MC generators, cross-section calculations and PDFs used are listed in Table~\ref{tab:montecarlo}.
For all SM background samples the response of the detector to particles is modelled with a full ATLAS detector simulation \cite{:2010wqa} based on \textsc{Geant4} \cite{Agostinelli:2002hh}.
Signal samples are prepared using a fast simulation based on a parameterization of the performance of the ATLAS electromagnetic and hadronic calorimeters \cite{ATLAS:2010bfa} and on \textsc{Geant4} elsewhere.
%, or through a fast simulation using a parameterization of the performance of the ATLAS electromagnetic and hadronic calorimeters \cite{ATLAS:2010bfa} and \textsc{Geant4} elsewhere; the latter applies to  \powheg{}+\pythia{} $t\bar{t}$ samples.

All simulated events are overlaid with multiple $pp$ collisions simulated with the soft QCD processes of \pythia~8.186 using the A2 tune  \cite{A14tune} and the MSTW2008LO parton distribution functions \cite{Martin:2009iq}. The simulations are reweighted to match the distribution of the mean number of interactions observed in data. %It was checked that the effect of such pile-up reweighting is completely negligible.

%The MC samples were generated with an expected pile-up distribution (multiple $pp$ interactions in the same or neighbouring bunch-crossings), but have not been reweighted to match the distribution of the mean number of interactions observed in data. It has been checked that the effect of such pile-up reweighting is completely negligible.

%Differing pile-up (multiple $pp$ interactions in the same or neighbouring bunch-crossings) conditions as a function of the instantaneous luminosity are taken into account by overlaying simulated minimum-bias events (simulated using \pythia~8 with the MSTW2008LO PDF set~\cite{Sherstnev:2007nd} and the A2 tune \cite{ATL-PHYS-PUB-2011-014})  onto the hard-scattering process and reweighting events according to the distribution of the mean number of interactions observed in data.

\section{Event selection}

This section describes the selection of events.
We begin by describing the \textit{preselection}, which is used to remove problematic events and reduce the dataset to a manageable size.
We then describe the signal region strategy, and present the signal regions used in the analysis.

\subsection{Preselection}

The preselection is used to reduce the dataset to that of interest in this thesis.
The table containing the preselection cuts is shown in \ref{tab:preselection}.
This selection is also used for the samples used for background estimation, except for the lepton veto.

The cuts [1] and [4] are a set of cleaning cuts to remove problematic events.
The \textit{Good Runs List} is a centrally-maintained list of data runs which have been determined to be ``good for physics''.
This determination is made by analysis of the various subdetectors, and monitoring of their status.
Event cleaning is used to veto events which could be affected by noncollision background, noise bursts, or cosmic rays.


We require the lowest unprescaled \met trigger for the data run of interest, as described previously, in cut [2].
The lepton veto is applied in cut [5].
These two cuts are only used for the signal region selection.

The rest of the preselection is used for the signal region and background estimation samples.
These cuts are mostly used for the reduction of the dataset to a manageable
Signal models with sensitivity to lower values of these scale variables have been ruled out by previous searches \todo{cite}.
The final cut is on \meff, which is the scalar sum of all jets and \met.
This is the final discriminating variable used in the complementary search to this thesis, which is also presented in \ref{ATLAS-CONF-2016-078}.

\input{Chapter-Analysis/preselection-table}

\subsection{Signal regions}
We define a set of of signal regions using the RJR variables previously described.
These signal regions are split into three general categories: squark pair production SRs, gluino pair production SRs, and compressed production SRs.
Within these general SRs, we have a set of signal regions targetting different mass splittings of the sparticle and LSP.
\begin{figure}
\caption{Schematic leading the development of the SUSY signal regions in this thesis.
A variant of this schematic is used for most SUSY searches on ATLAS and CMS.
} \label{fig:sr_schematic}
\includegraphics[width=.9\linewidth]{sr_schematic}
\end{figure}

A schematic of this strategy is shown in \ref{sr_schematic}.
This type of plane is how most ($R-$ parity conserving) SUSY searches are organized in both ATLAS and CMS.
The horizontal axis is the mass of the sparticle considered.
In the case of this thesis, this will the squark or gluino mass.
On the horizontal axis, we place the LSP mass.
These are the two free parameters of the simplified models considered here.
Our search occurs in this two-parameter space.
Each signal region targets some portion of this plane.
As shown in the figure, a new iteration of a search will use a set of signal regions which have sensitivity just beyond those of the previous exclusions.
The choice of how many signal regions to use to fully cover this plane is in many ways a matter of judgment, as it is essentially a matter of under/over-fitting to the signal models of interest.
One signal region will obscure the different phenomena in signal events with large versus small mass splittings, leading to underfitting.
Binning as finely as possible\footnotemark leads to overfitting due to the fluctuations present in the signal and background events passing this selection.
\footnotetext{This can be defined as having a signal region for each simulated signal sample, which for this analysis is \order 100.}
In this thesis, we use six squark signal regions, six gluino signal regions, and five compressed regions, which we describe below.

The full table defining all signal regions is shown in \ref{tab:RJsrdefs}.
In all cases, the signal region selections contain a combination of scaleful and scaleless cuts.
Emphasis on cuts on scaleful variables provide stronger sensitivity to larger mass splittings, while additional sensitivity to smaller mass splittings is found using stronger cuts on scaleless variables.
One envisions walking from SR1 (with tight scaleless cuts and loose scaleful cuts) in \ref{fig:sr_schematic} towards SR3 by loosening the scaleless cuts and tightening the scaleful cuts.
We will see this strategy at work in each set of signal regions.

We have already described the useful variables in the previous chapter.
The question is how to choose the optimal cuts for a given set of signal models, which are grouped in the mass splitting space.
This was done by a brute force scan over the cut values, using a guess of integrated luminosity with a fixed systematic uncertainty scenario, motivated by that from previous analyses.
We choose the lowest cut value that maximizes the $Z_{\text{Bi}}$, as described in \cite{Cousins:2008zz}.
This figure of merit gives conservative estimates, as compared to i.e. $S/\sqrt{B}$.
A figure showing an example of this selection tuning procedure is shown in \ref{fig:sr_optimization}.

\begin{figure}
\caption{Optimization of the \HTFnm{PP}{4}{1} cut for a gluino signal model with $(m_{\gluino}, m_{\lsp} ) = (1500,700) \GeV $ assuming 10 \ifb and an uncertainty of 20\% on the background estimate.
} \label{fig:sr_optimization}
\includegraphics[width=.9\linewidth]{ATLAS-CONF-2016-078_INT/OPT_gluino/HT5PP_10fb_20sys_gg1500_700}
\end{figure}

The compressed selections are split into five regions (SRC1-5), and due to the simplified nature of the compressed decay tree, has sensitivity in both the gluino and squark planes.
The compressed regions target mass splittings with $m_{\text{sparticle}} - m_{\text{LSP}} \tilde{<} 200 \GeV$.
For the compressed region, $M_{T, S}$ is the primary scaleful variable.
We can see the general strategy of lowering increasing scale cuts while decreasing the scaleless cuts here.
SRC1 targets the most compressed scenarios, with mass splittings of less than 25 \GeV, and has the loosest $M_{T, S}$ cut coupled with the tightest \risr and \dphiISR cuts.
SRC4 and SRC5 target mass splittings of $\tilde{~} 200 \GeV$, and are coupled with the loosest scaleless cuts on \risr and \dphiISR.
We also note that SRC4 and SRC5 have differing cuts on \NVjet, since these SRs are closest to the regions we will describe below, and can be see as the ``cross-over'' where the differences between squark and gluino production begins to become manifest.

The squark regions (for noncompressed spectra) are organized into six signal regions.
These are labeled by a numeral 1-3 and letter a/b.
SRs sharing a common numeral i.e. SRS1a and SRS1b share a common set of scaleless cuts, while differing in the main scale variable \HTFnm{PP}{2}{1}.
The two SRs for each set of scaless cuts, only differing in the main scale variable, can be seen in \todo{naive} way as providing sensitivity to a range of luminosity scenarios\footnotemark.
\footnotetext{These SRs were defined before the entire collision dataset was produced, and thus need to be robust in cases where the LHC provides significantly different than expected performance.}
As before, we see that the scaleless cuts are loosened as we tighten the scaleful cuts, as we move across the table from SRS1a to SRS3b.
This provides strong sensivity to signal models with intermediate mass splittings with SRS1a to large mass splittings with SR3b.

The gluino signal regions are organized entirely analogously to the squark signal regions.
There are six gluino signal regions, again labeled via a numeral 1-3 and letter a/b.
Those SRs sharing a common numeral have a common set of scaleless cuts, but differ in their main scale variable \HTFnm{PP}{4}{1}.
The SRs follow the strategy, with SRG1 having the loosest scaleful cut cuts coupled with the strongest scaleless cuts, and the converse being true in SRG3.
As in the squark case, this strategy provides strong expected sensitivity throughout the gluino-LSP plane.
\todo{PLOT expected sensitivity with these assumptions?}

\input{Chapter-Analysis/signal-region-table}

\subsection{Scale variable distributions in the signal regions}

In \ref{fig:srs_scale,fig:srg_scale,fig:src_scale}, we can see the distributions of the last scale cut used for each signal region.
These distributions include scale factors derived via the fitting procedure which are applied to the Standard Model background, which we will describe later in this chapter.
These scale factors are all $\order 1$.
The systematic uncertainties, shown in the banded stripe, are also described later in this section.
Each plot shows the distribution from a signal model which is targetted by the given signal region.

These distributions have all cuts applied except for the cut on this scale variable, which allows us to see the additional discrimination provided by the given variable.
Since signal regions with the same numeral have identical cuts on all cuts other than the main scale variable, we show (a) and (b) on the same figure.
The left-most (right-most) arrow shown is the location of the a (b) cut applied in the analysis.
We call these plot \textit{$N-1$} plots, where $N$ refers to the number of cuts applied in the analysis.
The full set of $N-1$ plots in the signal regions for the other variables used in the analysis are shown in \ref{app:n-1_plots}.
We can see that for the signal models targeted by each signal region, each cut provides unique discrimination against the Standard Model backgrounds.

\todo{captions for these figures}
\input{Chapter-Analysis/scale-cut-figures}

\section{Background estimation}

We describe here the method of background estimation.
In this thesis, we detail what is colloquially called a ``cut-and-count'' analysis.
This is in contrast to a ``shape fit'' analysis, where one needs to consider the details of the variable distribution shapes.
Instead, we must ensure the overall normalization of the Standard Model backgrounds are correct in the regions of phase space considered in the analysis.
In order to do this, we define a set of \textit{control regions} which are free of SUSY contamination based on the previously excluded analysis.
We compare the number of events present in the control regions in simulation with that in data to define a \textit{transfer factor} (TF).
We extrapolate the number of expected events from each background using this transfer factor to translate from the , which provides our final estimate of the SM background in the corresponding signal region.\todo{yes?}\footnotemark
\footnotetext{To be explicit, each signal region SR has a corresponding set of control regions.
}

More precisely, for a given signal region, we are attempting to estimate the value $\ntext{SR}{data}$ for a given background.
This value is estimated using the following equation:
\begin{align}\label{eq:bkg_est}
\ntext{SR}{data,est} = \ntext{CR}{data,obs} \times \text{TF}_{\text{CR}} \equiv \ntext{CR}{data,obs} \times  \begin{pmatrix} \frac{ \ntext{SR}{MC} }{ \ntext{CR}{MC} } \end{pmatrix}
\end{align}
where the transfer factor TF is taken directly from MC.
The two ingredients to our estimation of \ntext{SR}{data,obs} is thus \ntext{CR}{data,obs} and the transfer factor taken from MC.
The CR definitions are motivated and designed according to two (generally competing) requirements:
\begin{enumerate}
\item Statistical uncertainties due to low CR statistics
\item Systematic uncertainties related to the extrapolation from the CR to the SR.  This motivates the desire to make the control regions as similar as possible to the signal regions without risking signal contamination while ensuring high purity in the targeted SM background.
\end{enumerate}
In principle, we can also apply data-driven corrections to the TF obtained for each CR.

In order to validate the transfer factors obtained from MC, we also develop a series of \textit{validation regions} (VRs).
These regions are generally designed to be ``in between'' the control region and signal region selections in phase space, and thus provide a check on the extrapolation from the control regions into the signal regions.
Despite their closeness in phase space to the signal regions, they are also designed to have low signal contamination.

In practice, we perform this estimation procedure simultaneously across all control regions; we describe this later.
We only note this here since in principle, we can apply Eq.\ref{eq:bkg_est} to measure the contamination of backgrounds within control regions as well.
This procedure also accounts for the correlations between regions due to correlated systematic uncertainties.
We next describe the control region selection for the major SM backgrounds for the analysis.

\subsection{Control and Validation Regions}

As was hinted at in the discussion of Monte Carlo generators, the primary backgrounds of note in this analysis are \zjets,\wjets,\ttbar, and QCD events.
There is also a minor background from diboson events which is taken directly from MC with an uncertainty of 50\%.
We describe the strategy to estimate these various backgrounds here.
A summary table is shown in \ref{tab:crdefs}.
\input{Chapter-Analysis/background-estimation-table}

Events with a $Z$ boson decaying to neutrinos in association with jets are the primary irreducible background in the analysis.
These events have true \met from the decaying neutrinos, and can have significant values of the scale variables of interest.
Naively, one might expect us to use \Zll as the control process of interest, as \Zll events are quite well-measure.
Unfortunately, the \Zll branching ratio is about half of from \Zvv, which necessitates loosening the control region selection significantly.
This leads to large systematic uncertainties in the transfer factor to account for the separation in phase space.

Instead, photon events are used as the control for the \Zvv events.
We label this photon control region as CRY.
The photon is required to have $\pt > 150 \GeV$ to ensure the trigger is fully efficient.
The kinematic properties of photon events strongly resemble those of $Z$ events when the boson \pt is significantly above the mass of the $Z$ boson.
In this regime, the neutral boson properties are scaleless, and can be treated interchangeably, up to the differences in coupling strengths.
%There are some residual effects due to the differing spin states, which should be identifiable in events with
Additionally, the cross-section for \gammajets events is significantly larger than \zjets events above the $Z$ mass.
These features are shown in \ref{fig:boson_pt_ratio} in simulated \Zvv truth and reconstructed events.
The reconstructed \Zvv events define the boson \pt as simply the \met.
In truth events, one clearly sees the effect of the $Z$ mass below \order 100 \GeV, with a flattening of the ratio above \order 300 \GeV.
In reconstructed events, the effects are less clear at low boson \pt, primarily due to cut sculpting from i.e. the trigger requirement on photon events, which necessitates a higher \pt cut on photon events for the trigger to remain fully efficient.
Still, it is clear that the ratio flattens out at high boson \pt, and we are justified in the use of CRY to model the \zjets background.

The CRY kinematic selection is slightly looser in the scale variables for the noncompressed regions to provide sufficient control region statistics.
This is chosen to be $\HFnm{PP}{1}{1} > 900 \GeV$ ($\HFnm{PP}{1}{1} > 550 \GeV$) for the squark (gluino) regions to properly minimize the corresponding statistical and systematic uncertainties.
\begin{figure}
\caption{} \label{fig:boson_pt_ratio}
\subfloat[Boson \pt ratio as a function of true boson \pt]{\includegraphics[width=.45\linewidth]{ATLAS-CONF-2016-078_INT/GammaReweighting/Znunu_truth_bosonPt_dPhi/c1_bosonPt_no_cuts}}
\subfloat[Boson \pt ratio as a function of reconstructed boson \pt]{\includegraphics[width=.45\linewidth]{ATLAS-CONF-2016-078_INT/GammaReweighting/Znunu_reco/c1_bosonPt_no_cuts}}
\end{figure}

One additional correction scale factor is applied to \gammajets events before calculating the transfer factors.
This is known as the $\kappa$ method, which is used to determine the disagreement arising from the use of a LO generator for photon events vs. a NLO generator for \zjets events, which can reduce the theoretical uncertainties from this disagreement. \todo{cite what they are}
One can see this as a measurement of the k-factor for the LO \gammajets sample.
This is effectively done with an auxiliary CRZ region, defined using two leptons associated to the Z mass.
The correction factor derived for this purpose is $\kappa = 1.39 \pm 0.05$.

Event with a $W$ boson decaying leptonically via \wln can also enter the signal region.
In  case, we use leptonically to include all leptons ($e,\mu,\tau$).
The \wjets events passing the event selection either have a hadronically-decaying $\tau$, where the neutrino is used as \met, or the case where a muon or electron is misidentified as a jet or missed completely due to the limited detector acceptance.
To model this background, we use a sample of one-lepton events with a veto on b-jets.
The lepton is required to have $\pt > 27 \GeV$ to guarantee a fully efficient trigger.
We then treat this single lepton as a jet for purposes of the RJR variable calculations.
We apply a kinematic selection on the transverse mass:
\begin{align}
m_T = \sqrt{2p_{T,\ell} \met ( 1 - \cos{ \phi_{e} - \metphi } },
\end{align}
around the $W$ mass: $30 \GeV < m_T < 100 \GeV$.
Checks in simulation and experience from other analyses shows that these requirements give a sample of high purity \wln background.
Due to low statistics using the kinematic cuts imposed in the signal regions, the control region kinematic cuts are slightly loosened with respect to the signal region cuts.
We use the loosest cut in any signal region as the control region selection for all signal regions.
More clearly, the control region selection corresponding to each signal region is the \textit{same}.
As discussed above, this leads to a tolerable increase in the systematic uncertainty from the extrapolation from the CR to the SR when compared to the resulting statistical uncertainty.

Top events are also an important background, for the same reasons as the \wjets background, due to the dominant top decay channel of $t \rightarrow Wb$.
For a top event to be selected by the analysis criteria, as in the case of \wjets, we expect a $W$ to decay via a $\tau$ lepton which decays hadronically or one a muon or electron to be misidentified as a jet or be outside the detector acceptance.
We are not so worried about hadronic or all dileptonic tops: hadronic \ttbar events generally have low \met (and \HFnm{PP}{1}{1}) so they will not pass the kinematic cuts, while dileptonic \ttbar events have a lower cross-section and good reconstruction efficiency from the two leptons.
We are thus primarily concerned with semileptonic \ttbar events with \met from the neutrino.
To model this background, we use the same selection as the $W$ selection, but require that one of the jets chosen by the analysis has at least one $b$-tag.
This selection has quite high purity, as we expect the \ttbar background to have two $b-$jets.
Thus with the 70\% $b-$tagging efficiency working point used in this analysis, ignoring (small) correlations between the two $b-$tags, we expect to tag one of the $b-$jets greater than 90\% of the time.
As with CRW, we need to loosen the cuts applied to CRT with respect to the signal region in order to gain sufficient expected data statistics.
We use exactly the same scheme; the CRT corresponding to each SR is identical, due to using the loosest set of cuts among the SRs.
This comes at the cost of an increased systematic uncertainty for this extrapolation, but it was determined that this tradeoff resulted in the lowest overall uncertainty.

The final important background is the QCD background.
As briefly discussed in the previous chapter, QCD backgrounds are difficult, for a few reasons we describe here.
The large cross-section for QCD events means that even very rare extreme mismeasurements can be seen in our signal regions.
However, as these events are very rare, one requires extreme confidence in the tails of the distributions to use simulation as an input for background estimation.
To avoid this, the strategy in these cases is to apply a strong enough cut to expect \textit{zero} QCD events in the signal regions to avoid this issue.
To produce a sample enriched in QCD, which we call CRQ, we reverse the $\Delta_{\mathrm{QCD}}$ and \HFnm{PP}{1}{1} cuts.
This analysis uses the jet smearing method.

Validation region definitions

\subsection{R Z/$\gamma$ method}

\subsection{Systematic Uncertainties}
